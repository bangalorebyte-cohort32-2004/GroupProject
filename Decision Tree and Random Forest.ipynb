{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cohort Number : 33 <br>\n",
    "> Lecture Number : 10 <br>\n",
    "> Author : Jithin J Kumar <br>\n",
    "> Topic : Data Preparation Process <br>\n",
    "> Date : 20-04-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Algorithms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Decision Tree ? How does it work ?**\n",
    "- Decision tree is a type of supervised learning algorithm (having a predefined target variable) that is mostly used in classification problems. \n",
    "- It works for both categorical and continuous input and output variables. \n",
    "- In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**\n",
    "- The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree. \n",
    "- In ZeroR model there is no predictor, in OneR model we try to find the single best predictor, naive Bayesian includes all predictors using Bayes' rule and the independence assumptions between predictors but decision tree includes all predictors with the dependence assumptions between predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy**\n",
    "- A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). \n",
    "- ID3 algorithm uses entropy to calculate the homogeneity of a sample. \n",
    "- If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain**\n",
    "- The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    " - [Decision Tree from Saed Sayad Blog](https://www.saedsayad.com/decision_tree.htm)\n",
    " - [ISLR Chapter 8](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well these concepts are so well explained by Trevor Hastie (Author of ISLR)\n",
    "- [Trevor Hastie - Tree Based Learning](https://www.youtube.com/watch?v=wPqtzj5VZus)\n",
    "- [Presentation](https://www.cc.gatech.edu/~hic/CS7616/pdf/lecture5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Entropy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -p log p - q logq\n",
    "import math\n",
    "\n",
    "p = 9/14\n",
    "q = 5/14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_y = (-p*math.log(p,2))-(q*math.log(q,2))\n",
    "\n",
    "def entropy(a,b):\n",
    "    p = a/(a+b)\n",
    "    q = b/(a+b)\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return (-p*math.log(p,2))-(q*math.log(q,2))\n",
    "\n",
    "def probability(p, total):\n",
    "    return p/total\n",
    "\n",
    "P = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2467498197744391"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entropy_outlook = P(5,14)*entropy(3,2) + P(4,14)*entropy(4,0) + P(5,14)*entropy(2,3)\n",
    "gain =  Entropy_y - Entropy_outlook\n",
    "gain\n",
    "\n",
    "gini = 1- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Decision Tree\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [25, 30, 46, 45, 52, 43, 23, 35, 38, 46, 48, 52, 44, 30]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = len(y)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.321086474291743"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_prob(p,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
